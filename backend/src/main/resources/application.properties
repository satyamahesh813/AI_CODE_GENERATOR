server.port=8081
spring.application.name=orchestrator

# LLM Configuration
llm.provider=huggingface

# Ollama Configuration (Disabled - too weak for complex code generation)
# If running locally vs Docker, adjust host (localhost vs host.docker.internal)
# ollama.api.url=http://localhost:11434/api/chat
# ollama.model=llama3

# OpenRouter (Highly Reliable Gateway - Disabled)
# Get a key at https://openrouter.ai/keys
# openrouter.api.key=
#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

# Switched to Llama 3 Free (Most reliable free model)
# openrouter.model=
#meta-llama/llama-3-8b-instruct:free

# Other Options:
# google/gemini-flash-1.5 (Correct ID - Paid/Low Cost)
# google/gemini-2.0-flash-exp:free (Free but Rate Limited)
# microsoft/phi-3-mini-128k-instruct:freebf04d334de3eb3a161ad297df22479dfc5b

# Gemini (Direct API)
#gemini.api.key=
#AIzaSyCubhLPYhmrqssjJhmakgojLpTWzbDhGh0
#gemini.model=
#gemini-1.5-flash

# HuggingFace Inference API (OpenAI-compatible format)
# Get a free API key at https://huggingface.co/settings/tokens
huggingface.api.key=${HUGGINGFACE_API_KEY:sk-proj-fake-key-for-dev}
huggingface.api.url=${HUGGINGFACE_API_URL:https://router.huggingface.co/v1}
huggingface.model=${HUGGINGFACE_MODEL:meta-llama/Meta-Llama-3-8B-Instruct}
#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
#huggingface.api.url=
#https://router.huggingface.co/v1
#huggingface.model=
#meta-llama/Meta-Llama-3-8B-Instruct
#huggingface.model=google/flan-t5-base
#meta-llama/Meta-Llama-3-8B-Instruct
# Other models: Qwen/Qwen2.5-Coder-32B-Instruct, deepseek-ai/DeepSeek-R1
# Add :fastest or :cheapest suffix for provider selection (e.g., meta-llama/Meta-Llama-3-8B-Instruct:fastest)

# OpenAI (Quota Limited)
#llm.api.base=https://api.openai.com/v1
#llm.api.key=YOUR_OPENAI_KEY
#llm.model=
#gpt-3.5-turbo
